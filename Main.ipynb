{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOemg0bOkYZUQVxhmWFA1VT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahassanzadeh/Multi-Armed-Bandits/blob/master/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUqfekD0FEKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import modules \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzOYdT4138pX",
        "colab_type": "text"
      },
      "source": [
        "# **Epsilon Greedy** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqi6H0lBDqAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class greedy_bandit:\n",
        "    def __init__(self, k, epsilon, iterations, rew):\n",
        "        # Number of arms(actions)\n",
        "        self.k = k\n",
        "        # Search probability\n",
        "        self.epsilon = epsilon\n",
        "        # Number of iterations\n",
        "        self.iterations = iterations\n",
        "        # Step count\n",
        "        self.n = 0\n",
        "        # Step count for each arm\n",
        "        self.k_n = np.zeros(k)\n",
        "        # Total mean reward\n",
        "        self.mean_reward = 0\n",
        "        self.reward = np.zeros(iterations)\n",
        "        \n",
        "        if rew == \"greedy\":\n",
        "          # Reward per step \n",
        "          self.rew = np.random.normal(0, 1, k)\n",
        "        elif rew == 'Optimistic_Initial_Value':\n",
        "          # Select initial values\n",
        "          self.rew = np.repeat(5., k)\n",
        "        # Mean reward for each arm\n",
        "        self.k_reward = np.zeros(k)\n",
        "\n",
        "                                 \n",
        "    def choose_action(self):\n",
        "        # Generate random number\n",
        "        p = np.random.rand()\n",
        "        if self.epsilon == 0 and self.n == 0:\n",
        "            a = np.random.choice(self.k)\n",
        "        elif p < self.epsilon:\n",
        "            # Randomly select an action\n",
        "            a = np.random.choice(self.k)\n",
        "        else:\n",
        "            # Take greedy action\n",
        "            a = np.argmax(self.k_reward)\n",
        "            \n",
        "        reward = np.random.normal(self.rew[a], 1)\n",
        "        \n",
        "        # Update counts\n",
        "        self.n += 1\n",
        "              \n",
        "        # Update total\n",
        "        self.mean_reward = self.mean_reward + (\n",
        "            reward - self.mean_reward) / self.n\n",
        "\n",
        "        # Update results for a_k\n",
        "        self.k_reward[a] = self.k_reward[a] + (\n",
        "            reward - self.k_reward[a]) / self.k_n[a]\n",
        "\n",
        "    def run(self):\n",
        "        for i in range(self.iterations):\n",
        "            self.choose_action()\n",
        "            self.reward[i] = self.mean_reward\n",
        "    def reset(self):\n",
        "        # Resets results while keeping settings\n",
        "        self.n = 0\n",
        "        self.k_n = np.zeros(k)\n",
        "        self.mean_reward = 0\n",
        "        self.reward = np.zeros(iterations)\n",
        "        self.k_reward = np.zeros(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0zI1z4BDtl5",
        "colab_type": "code",
        "outputId": "5e208f59-3ea2-4099-8342-fe8f5dac93e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "k = 10\n",
        "iterations = 1000\n",
        "\n",
        "eps_0_rewards = np.zeros(iterations)\n",
        "eps_01_rewards = np.zeros(iterations)\n",
        "eps_1_rewards = np.zeros(iterations)\n",
        "\n",
        "episodes = 1000\n",
        "# Run experiments\n",
        "for i in range(episodes):\n",
        "    # Initialize bandits\n",
        "    eps_0  = greedy_bandit(k, 0, iterations,\"greedy\")\n",
        "    eps_01 = greedy_bandit(k, 0.01, iterations, eps_0.rew.copy())\n",
        "    eps_1  = greedy_bandit(k, 0.1, iterations, eps_0.rew.copy())\n",
        "    # Run experiments\n",
        "    eps_0.run()\n",
        "    eps_01.run()\n",
        "    eps_1.run()\n",
        "    \n",
        "    # Update long-term averages\n",
        "    eps_0_rewards = eps_0_rewards + (\n",
        "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
        "    eps_01_rewards = eps_01_rewards + (\n",
        "        eps_01.reward - eps_01_rewards) / (i + 1)\n",
        "    eps_1_rewards = eps_1_rewards + (\n",
        "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
        "\n",
        "plt.figure(figsize=(16,12))\n",
        "plt.plot(eps_0_rewards, label=\"$\\epsilon=0$ (greedy)\")\n",
        "plt.plot(eps_01_rewards, label=\"$\\epsilon=0.01$\")\n",
        "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
        "plt.legend(bbox_to_anchor=(1.3, 1))\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Average $\\epsilon-greedy$ Rewards after \" + str(episodes) + \" Episodes\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fa7e34af2ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0meps_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0meps_01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0meps_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-529a4f35247d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-529a4f35247d>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Update counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'greedy_bandit' object has no attribute 'rew'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghsFZbPb7-YF",
        "colab_type": "text"
      },
      "source": [
        "# **Optimistic Initial Value(OIV)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlhOhdORDwTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k = 10\n",
        "# iters = 1000\n",
        "\n",
        "# eps_01_rewards = np.zeros(iters)\n",
        "\n",
        "\n",
        "\n",
        "# # Run experiments\n",
        "# for i in range(iters):\n",
        "#     # Initialize bandits\n",
        "#     eps_1 = eps_bandit(k, 0.1, iters)\n",
        "#     # Run experiments\n",
        "#     eps_1.run()\n",
        "    \n",
        "#     # Update long-term averages\n",
        "#     eps_01_rewards = eps_01_rewards + (\n",
        "#         eps_01.reward - eps_01_rewards) / (i + 1)\n",
        "#     oiv_rewards = oiv_rewards + (\n",
        "#         oiv_bandit.reward - oiv_rewards) / (i + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}